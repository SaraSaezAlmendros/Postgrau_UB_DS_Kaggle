{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: SMS: SPAM or HAM (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5964, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speead up development. \n",
    "# Modify for final system\n",
    "#data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide the training and test set into two partitions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val, label_train, label_val = \\\n",
    "    train_test_split(data, data[\"label\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>The current revcon document includes a paragra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>Sorry to confirm they are not in.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>From Israel RadioBy Shmuel Tal\"Prime Minister ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "5650                                                 Ok      0\n",
       "213   The current revcon document includes a paragra...      0\n",
       "2084                                                fyi      0\n",
       "4785                  Sorry to confirm they are not in.      0\n",
       "4314  From Israel RadioBy Shmuel Tal\"Prime Minister ...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()\n",
    "#print(label_train)\n",
    "# print(label_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n",
      "WN: sleep\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "# tal vegada WordNet millor\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemma  = WordNetLemmatizer()\n",
    "\n",
    "print('WN:',wordnet_lemma.lemmatize('sleeping',pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>The current revcon document includes a paragra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>Sorry to confirm they are not in.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>From Israel RadioBy Shmuel Tal\"Prime Minister ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "5650                                                 Ok      0\n",
       "213   The current revcon document includes a paragra...      0\n",
       "2084                                                fyi      0\n",
       "4785                  Sorry to confirm they are not in.      0\n",
       "4314  From Israel RadioBy Shmuel Tal\"Prime Minister ...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>The current revcon document includes a paragra...</td>\n",
       "      <td>0</td>\n",
       "      <td>current revcon document include paragraph nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>Sorry to confirm they are not in.</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry confirm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>From Israel RadioBy Shmuel Tal\"Prime Minister ...</td>\n",
       "      <td>0</td>\n",
       "      <td>israel radioby shmuel tal prime minister explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "5650                                                 Ok      0   \n",
       "213   The current revcon document includes a paragra...      0   \n",
       "2084                                                fyi      0   \n",
       "4785                  Sorry to confirm they are not in.      0   \n",
       "4314  From Israel RadioBy Shmuel Tal\"Prime Minister ...      0   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "5650                                                 ok  \n",
       "213   current revcon document include paragraph nort...  \n",
       "2084                                                fyi  \n",
       "4785                                     sorry confirm   \n",
       "4314  israel radioby shmuel tal prime minister explo...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "punctuation_pattern = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0]#.upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_text(text):\n",
    "    processed_feature = text\n",
    "    \n",
    "    # Remove inline JavaScript/CSS:\n",
    "    #processed_feature = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \" \", processed_feature)    \n",
    "    \n",
    "    # Remove html comments\n",
    "    #processed_feature = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \" \", processed_feature)\n",
    "    \n",
    "    # Remove remaining html tags. (Added flag if \"<div\" is found later on process?)\n",
    "    #processed_feature = re.sub('<.*?>', ' ', processed_feature)\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(processed_feature))\n",
    "\n",
    "    # remove all single characters\n",
    "    #processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    #processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punctuation_pattern.sub(' ', processed_feature)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "    # Remove encoded symbols like =2E, =3A....\n",
    "    #processed_feature = re.sub(r'=?\\d[a-z]', ' ', processed_feature)\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    list_text = processed_feature.split(' ')\n",
    "    lemmatized_text = []\n",
    "    for word in list_text:\n",
    "        clean_word = wordnet_lemma.lemmatize(word, get_wordnet_pos(word))\n",
    "        if clean_word not in stopwords.words(\"english\"):\n",
    "            lemmatized_text.append(clean_word)\n",
    "        \n",
    "    processed_feature = ' '.join(lemmatized_text)\n",
    "    \n",
    "    spam_words = [\"urgent\", \"2c\", \"2e\", \"private\", \"html\", \"profitable\", \"land dispute\"]\n",
    "    for spam_word in spam_words:\n",
    "        processed_feature = re.sub(fr'{spam_word}', f'{spam_word} {spam_word} {spam_word} {spam_word} {spam_word} {spam_word} {spam_word}', processed_feature)\n",
    "    \n",
    "    # Remove numbers\n",
    "    processed_feature = re.sub(r'\\d', ' ', processed_feature)\n",
    "    \n",
    "    return processed_feature\n",
    "\n",
    "\n",
    "data_train.loc[:,'preprocessed_text'] = data_train['text'].apply(clean_text)\n",
    "data_val.loc[:,'preprocessed_text'] = data_val['text'].apply(clean_text)\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>68222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>38098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private</td>\n",
       "      <td>5835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urgent</td>\n",
       "      <td>4616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>money</td>\n",
       "      <td>3827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>br</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>u</td>\n",
       "      <td>3479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>account</td>\n",
       "      <td>3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bank</td>\n",
       "      <td>3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fund</td>\n",
       "      <td>3042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "0        e  68222\n",
       "1        c  38098\n",
       "2  private   5835\n",
       "3   urgent   4616\n",
       "4    money   3827\n",
       "5       br   3533\n",
       "6        u   3479\n",
       "7  account   3366\n",
       "8     bank   3240\n",
       "9     fund   3042"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_ham  = data_train[data_train['label'] == 0].copy()\n",
    "data_spam = data_train[data_train['label'] == 1].copy()\n",
    "\n",
    "words_data_ham  = data_ham['preprocessed_text']\n",
    "words_data_spam = data_spam['preprocessed_text']\n",
    "\n",
    "\n",
    "list_ham_words = []\n",
    "for sublist in words_data_ham:\n",
    "    for item in sublist.split():\n",
    "        list_ham_words.append(item)\n",
    "\n",
    "list_spam_words = []\n",
    "for sublist in words_data_spam:\n",
    "    for item in sublist.split():\n",
    "        list_spam_words.append(item)\n",
    "        \n",
    "c_ham  = Counter(list_ham_words)\n",
    "c_spam = Counter(list_spam_words)\n",
    "df_hamwords_top10  = pd.DataFrame(c_ham.most_common(10),  columns=['word', 'count'])\n",
    "df_spamwords_top10 = pd.DataFrame(c_spam.most_common(10), columns=['word', 'count'])\n",
    "\n",
    "df_spamwords_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def lemmatize(word, pos='v'):\n",
    "    return wordnet_lemma.lemmatize(word, pos=pos)\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    out = word_tokenize(text)\n",
    "    out = [lemmatize(word,\n",
    "#                      pos=\"v\"\n",
    "                    ) for word in out]\n",
    "    out = [word for word in out if word not in stop]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#bow_transformer = CountVectorizer().fit(data_train['preprocessed_text'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,4))#, tokenizer=tokenize_lemmatize) #ngram_range=(1,4): paraules en cadena de 4\n",
    "bow_transformer = tfidf_vectorizer.fit(data_train['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039785\n",
      "nora cheryl email dozen memo haiti weekend please print organize inchrono order trip tomorrow send lauren thx \n",
      "  (0, 984916)\t0.13943395159016175\n",
      "  (0, 984915)\t0.13943395159016175\n",
      "  (0, 984914)\t0.13943395159016175\n",
      "  (0, 984867)\t0.09574974695093985\n",
      "  (0, 920774)\t0.13943395159016175\n",
      "  (0, 920773)\t0.13943395159016175\n",
      "  (0, 920772)\t0.13943395159016175\n",
      "  (0, 920532)\t0.0739339755090635\n",
      "  (0, 906026)\t0.13943395159016175\n",
      "  (0, 906025)\t0.13943395159016175\n",
      "  (0, 906024)\t0.13943395159016175\n",
      "  (0, 905811)\t0.07247253535014989\n",
      "  (0, 897642)\t0.08254938766062028\n",
      "  (0, 810871)\t0.13943395159016175\n",
      "  (0, 810870)\t0.13943395159016175\n",
      "  (0, 810295)\t0.04856664610860974\n",
      "  (0, 719029)\t0.13943395159016175\n",
      "  (0, 719028)\t0.13943395159016175\n",
      "  (0, 719027)\t0.13943395159016175\n",
      "  (0, 718956)\t0.07681951881467543\n",
      "  (0, 699645)\t0.13943395159016175\n",
      "  (0, 699644)\t0.13943395159016175\n",
      "  (0, 699640)\t0.12825262885531874\n",
      "  (0, 698430)\t0.04033794301342055\n",
      "  (0, 664896)\t0.13943395159016175\n",
      "  :\t:\n",
      "  (0, 570999)\t0.13943395159016175\n",
      "  (0, 570998)\t0.13943395159016175\n",
      "  (0, 570944)\t0.08719005588711921\n",
      "  (0, 516390)\t0.13943395159016175\n",
      "  (0, 516367)\t0.09869081953293518\n",
      "  (0, 443284)\t0.13943395159016175\n",
      "  (0, 443283)\t0.13943395159016175\n",
      "  (0, 443282)\t0.13943395159016175\n",
      "  (0, 443281)\t0.13943395159016175\n",
      "  (0, 397740)\t0.13943395159016175\n",
      "  (0, 397739)\t0.13943395159016175\n",
      "  (0, 397738)\t0.13943395159016175\n",
      "  (0, 397507)\t0.09280867436894452\n",
      "  (0, 287855)\t0.13943395159016175\n",
      "  (0, 287854)\t0.13943395159016175\n",
      "  (0, 287853)\t0.13943395159016175\n",
      "  (0, 286979)\t0.04437216618667992\n",
      "  (0, 264925)\t0.13943395159016175\n",
      "  (0, 264924)\t0.13943395159016175\n",
      "  (0, 264923)\t0.13943395159016175\n",
      "  (0, 264891)\t0.10693106968578286\n",
      "  (0, 156217)\t0.13943395159016175\n",
      "  (0, 156216)\t0.13943395159016175\n",
      "  (0, 156215)\t0.13943395159016175\n",
      "  (0, 156163)\t0.07632817406324913\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_transformer.vocabulary_))\n",
    "sample_spam = data_train['preprocessed_text'][2]\n",
    "bow_sample_spam = bow_transformer.transform([sample_spam])\n",
    "\n",
    "# Lets look at some vectorization example for a spam email\n",
    "print(sample_spam)\n",
    "print(bow_sample_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4174, 1039785)\n",
      "(1790, 1039785)\n"
     ]
    }
   ],
   "source": [
    "## let's vectorize all dataset\n",
    "X_train = bow_transformer.transform(data_train['preprocessed_text'])\n",
    "X_val  = bow_transformer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "#Let's print the shape of the vectorized dataset\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9692737430167597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[949,  55],\n",
       "       [  0, 786]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "#Learn Classifier\n",
    "clf = MultinomialNB().fit(X_train, label_train)\n",
    "#Predict Val data\n",
    "pred_val = clf.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(label_val, pred_val)\n",
    "print(accuracy)\n",
    "confusion_matrix(label_val, pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TASK - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons.\n",
    "\n",
    "**Deadline**: 09/01/2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/kg_test.csv\",encoding='latin-1')\n",
    "X_test = bow_transformer.transform(data_test['text'].apply(clean_text))\n",
    "pred_text = clf.predict(X_test)\n",
    "\n",
    "submission_file = pd.DataFrame({'Id': data_test.index,'Category':pred_text})\n",
    "submission_file.to_csv('data/to_submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Alejandro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so the score is:\n",
      "[0.36363636 0.5        0.8        0.71428571 0.57142857]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = None\n",
    "\n",
    "# %pip install nltk sklearn\n",
    "import nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "wordnet_lemma  = WordNetLemmatizer()\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "def lemmatize(word, pos='v'):\n",
    "    return wordnet_lemma.lemmatize(word, pos=pos)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def pre_process(text):\n",
    "    text = re.sub('<.*?>', ' ', text) # Remove html tags. Added flag if \"<div\" is found later on process\n",
    "    text = re.sub(r'=\\d\\w',' ',text) # Remove encoded symbols like =2E, =3A....    \n",
    "    return text\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    out = word_tokenize(text)\n",
    "    out = [lemmatize(word,\n",
    "#                      pos=\"v\"\n",
    "                    ) for word in out]\n",
    "    out = [word for word in out if word not in stop]\n",
    "    return out\n",
    "\n",
    "def process_data(df, vectorizer, normalizer, fit=False):\n",
    "    x_text = df['text'].apply(pre_process)\n",
    "    x_len = np.array(df['text'].apply(len)).reshape(-1, 1)\n",
    "    if fit:\n",
    "        vectorizer.fit(x_text)\n",
    "        normalizer.fit(x_len)\n",
    "    x_text = vectorizer.transform(x_text)\n",
    "    x_len = normalizer.transform(x_len)\n",
    "    \n",
    "    x_html = np.array(df['text'].str.contains('<div', case=False)).reshape(-1, 1)\n",
    "    X = hstack((x_text, x_len, x_html))\n",
    "    if 'label' in df:\n",
    "        Y = df['label']\n",
    "    else:\n",
    "        Y = None\n",
    "    return X, Y\n",
    "               \n",
    "    \n",
    "    \n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1').sample(100)\n",
    "\n",
    "\n",
    "# Transform Data\n",
    "# Change vectorizer for testing if required\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), tokenizer=tokenize_lemmatize)\n",
    "# vectorizer = CountVectorizer(tokenizer=tokenize_lemmatize, ngram_range=(1,4))\n",
    "\n",
    "# Normalizer for the email lenght\n",
    "normalizer = Normalizer()\n",
    "\n",
    "X, Y = process_data(data, vectorizer, normalizer, fit=True)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print('so the score is:')\n",
    "print(cross_validate(clf, X, Y, n_jobs=4, cv=5, scoring='f1')['test_score'])\n",
    "\n",
    "# Train\n",
    "clf.fit(X, Y)\n",
    "\n",
    "test_data = pd.read_csv(\"data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "X_test, Y_test = process_data(test_data, vectorizer, normalizer)\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "\n",
    "df = pd.DataFrame(result, columns=['Category'])\n",
    "df['Id'] = df.index\n",
    "df = df[['Id','Category']]\n",
    "df.to_csv('to_submit.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, Y)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
